{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Neuralink Compression Challenge](https://content.neuralink.com/compression-challenge/README.html)\n",
        "\n",
        "content.neuralink.com/compression-challenge/data.zip is one hour of raw electrode recordings from a Neuralink implant.\n",
        "\n",
        "This Neuralink is implanted in the motor cortex of a non-human primate, and recordings were made while playing a video game, like this.\n",
        "\n",
        "Compression is essential: N1 implant generates ~200Mbps of eletrode data (1024 electrodes @ 20kHz, 10b resolution) and can transmit ~1Mbps wirelessly.\n",
        "So > 200x compression is needed.\n",
        "Compression must run in real time (< 1ms) at low power (< 10mW, including radio).\n",
        "\n",
        "Neuralink is looking for new approaches to this compression problem, and exceptional engineers to work on it.\n",
        "If you have a solution, email compression@neuralink.com\n",
        "\n",
        "Leaderboard\n",
        "\n",
        "Name\tCompression ratio\tCompressed size\t./encode size\t./decode size\n",
        "zip\t2.2\t63M\t231K\t480K\n",
        "\n",
        "Task\n",
        "\n",
        "Build executables ./encode and ./decode which pass eval.sh. This verifies compression is lossless and measures compression ratio.\n",
        "\n",
        "Your submission will be scored on the compression ratio it achieves on a different set of electrode recordings.\n",
        "Bonus points for optimizing latency and power efficiency\n",
        "\n",
        "Submit with source code and build script. Should at least build on Linux.\n",
        "\n",
        "Data\n",
        "\n",
        "$ ls -lah data/\n",
        "total 143M\n",
        "193K 0052503c-2849-4f41-ab51-db382103690c.wav\n",
        "193K 006c6dd6-d91e-419c-9836-c3f320da4f25.wav\n",
        "...\n",
        "\n",
        "Uncompressed monochannel WAV files.\n",
        "5 seconds per file."
      ],
      "metadata": {
        "id": "nvE6XvUElS_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "yjNJL-ulEGCQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gj2awcX1Y3U"
      },
      "outputs": [],
      "source": [
        "! wget https://content.neuralink.com/compression-challenge/data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://content.neuralink.com/compression-challenge/eval.sh"
      ],
      "metadata": {
        "id": "rMGSt6WVQEpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip data.zip"
      ],
      "metadata": {
        "id": "uvAXnPwz1zHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary encoding of samples and cardinality\n",
        "\n",
        "Documentation says that sample size is 10bit.\n",
        "However, WAV files are encoded with sample size 16bit.\n",
        "This leads to believe that 6 bits are not utilized.\n",
        "\n",
        "- WAV: 16bit integer PCM, int16, -32768, +32767\n",
        "- real: 10bit, max uint10 = 2^10 - 1 = 1024 - 1 = 0b0011_1111_1111  0x3FF\n",
        "\n",
        "\n",
        "Moreover, basic approach is to construct frequency based conding scheme for possible binary numbers."
      ],
      "metadata": {
        "id": "kV20dOwEDEsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from scipy.io import wavfile\n",
        "\n",
        "def get_sample_unique_counts():\n",
        "  sample_count = dict()\n",
        "  num_samples_total = 0\n",
        "\n",
        "  for fname in os.listdir(\"data\"):\n",
        "      sample_rate, samples = wavfile.read(os.path.join(\"data\", fname))\n",
        "\n",
        "      for q in samples:\n",
        "          num_samples_total += 1\n",
        "          if not q in sample_count:\n",
        "              sample_count[q] = 0\n",
        "          sample_count[q] +=1\n",
        "\n",
        "  return sample_count, num_samples_total\n",
        "\n",
        "sample_unique_counts, num_samples_total = get_sample_unique_counts()\n",
        "print(\"num_samples_unique\", len(sample_unique_counts), \"num_samples_total\", num_samples_total)"
      ],
      "metadata": {
        "id": "aMRAUcxvAb49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "03_m_M1SvVRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_samples_binary(sample_unique_count: dict[int, int]):\n",
        "  vals = sorted([(k,v) for k,v in sample_unique_count.items()], key=lambda x: x[0])\n",
        "  for k,v in vals:\n",
        "    print(np.binary_repr(k, width=16), v)\n",
        "\n",
        "print_samples_binary(sample_unique_counts)"
      ],
      "metadata": {
        "id": "OK8m2X6lC4MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_samples_binary_hist(sample_unique_count: dict[int, int]):\n",
        "  vals = sorted([(k,v) for k,v in sample_unique_count.items()], key=lambda x: x[1], reverse=True)\n",
        "  for k,v in vals:\n",
        "    print(np.binary_repr(k, width=16), v)\n",
        "\n",
        "print_samples_binary_hist(sample_unique_counts)"
      ],
      "metadata": {
        "id": "CXreZAgpKk1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bit_set_frequency(sample_unique_counts: dict[int, int]) -> dict[int, int]:\n",
        "    count = dict()\n",
        "    for k,v in sample_unique_counts.items():\n",
        "        for i in range(16):\n",
        "          b = (1 << i)\n",
        "          if (k & b) != 0:\n",
        "            if not b in count:\n",
        "              count[b] = 0\n",
        "            count[b] += v\n",
        "    return count\n",
        "\n",
        "print_samples_binary(get_bit_set_frequency(sample_unique_counts))"
      ],
      "metadata": {
        "id": "-7nB9P2QXxS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def samples_to_bits(samples: np.array):\n",
        "  v = np.zeros((samples.shape[0], 16), dtype=np.uint8)\n",
        "  for i, sample in enumerate(samples):\n",
        "    for j in range(16):\n",
        "      b = (1 << j)\n",
        "      if (sample & b) != 0:\n",
        "        v[i,j] = 1\n",
        "  return v\n",
        "\n",
        "fname = \"a90f5eca-fdbe-4a21-94bd-c4c7f32fe365.wav\"\n",
        "sample_rate, samples = wavfile.read(os.path.join(\"data\", fname))\n",
        "samples_bits = samples_to_bits(samples)\n",
        "for q in samples_bits[:100]:\n",
        "  print(q)"
      ],
      "metadata": {
        "id": "FeODa3Hzn70-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def len_sequence_same_per_bit(samples: np.array):\n",
        "  counts = [[dict() for q in range(16)], [dict() for q in range(16)]]\n",
        "  prev_bit = [0 for q in range(16)]\n",
        "  count = [-1 for q in range(16)]\n",
        "\n",
        "  for s in samples:\n",
        "    for i,b in enumerate(s):\n",
        "      if b == prev_bit[i]:\n",
        "        if count[i] == -1:\n",
        "          count[i] = 0\n",
        "        count[i] += 1\n",
        "      else:\n",
        "        if count[i] > 0:\n",
        "          if not count[i] in counts[b][i]:\n",
        "            counts[b][i][count[i]] = 0\n",
        "          counts[b][i][count[i]] += 1\n",
        "          count[i] = 1\n",
        "          prev_bit[i] = b\n",
        "\n",
        "  return counts\n",
        "\n",
        "cont_seq_lens = len_sequence_same_per_bit(samples_bits)\n",
        "\n",
        "print(\"cont seq of zeroes lengths count:\")\n",
        "for i in range(16):\n",
        "  print(i, sorted([(k,v) for k,v in cont_seq_lens[0][i].items()], key=lambda x: x[0]))\n",
        "\n",
        "print(\"cont seq of ones lengths count:\")\n",
        "for i in range(16):\n",
        "  print(i, sorted([(k,v) for k,v in cont_seq_lens[1][i].items()], key=lambda x: x[0]))"
      ],
      "metadata": {
        "id": "QakWQEyoxOvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "for b in [0,1]:\n",
        "  fig = go.Figure()\n",
        "  fig.update_layout(title=f'total number of bits in sequences of \"{b}\" by length', xaxis_title='len', yaxis_title='count')\n",
        "\n",
        "  for i in range(16):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for l, count in cont_seq_lens[b][i].items():\n",
        "      x.append(l)\n",
        "      y.append(count * l)\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=x, y=y, mode ='markers', name=f\"bit{i}\"))\n",
        "\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "81kSWVCU6RY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_bits_in_seq_longer_than(counts: list[dict[int, int]], min_len: int = 16) -> int:\n",
        "  sum = 0\n",
        "  for q in counts:\n",
        "    for l, count in q.items():\n",
        "      if l >= min_len:\n",
        "        sum += count * l\n",
        "  return sum\n",
        "\n",
        "nb = 16\n",
        "num_total_bits = 16 * len(samples)\n",
        "num_bits_in_seq_longer_than_nb_0 = num_bits_in_seq_longer_than(cont_seq_lens[0], min_len=nb)\n",
        "num_bits_in_seq_longer_than_nb_1 = num_bits_in_seq_longer_than(cont_seq_lens[1], min_len=nb)\n",
        "\n",
        "print(\"num_total_bits\", num_total_bits)\n",
        "print(f\"num_bits_in_seq_longer_than({nb}) for zeroes:\", num_bits_in_seq_longer_than_nb_0, num_bits_in_seq_longer_than_nb_0 / num_total_bits)\n",
        "print(f\"num_bits_in_seq_longer_than({nb}) for ones:\", num_bits_in_seq_longer_than_nb_1, num_bits_in_seq_longer_than_nb_1 / num_total_bits)"
      ],
      "metadata": {
        "id": "gFiij3qx5HEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def changed_bits(samples: np.array):\n",
        "  counts = [0]\n",
        "\n",
        "  for i, s in enumerate(samples):\n",
        "    if i == 0:\n",
        "      continue\n",
        "\n",
        "    num_diff = 0\n",
        "    for j in range(16):\n",
        "      if samples[i-1][j] != s[j]:\n",
        "        num_diff += 1\n",
        "\n",
        "    counts.append(num_diff)\n",
        "\n",
        "  return np.array(counts)\n",
        "\n",
        "samples_changed_bits = changed_bits(samples_bits)\n",
        "\n",
        "import plotly.express as px\n",
        "df = px.data.tips()\n",
        "fig = px.histogram(samples_changed_bits, histnorm='probability density')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "dJQRy0g1D-7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "def get_changed_masks(samples):\n",
        "  for i, s in enumerate(samples):\n",
        "    if i == 0:\n",
        "      continue\n",
        "    m = samples[i-1] ^ s\n",
        "    yield (m, samples[i-1] & m, s & m)\n",
        "\n",
        "def count_all_changed_masks():\n",
        "    for fname in os.listdir(\"data\"):\n",
        "      sample_rate, samples = wavfile.read(os.path.join(\"data\", fname))\n",
        "      for s in get_changed_masks(samples):\n",
        "        yield s\n",
        "\n",
        "changed_masks = Counter(count_all_changed_masks())\n",
        "print(\"num_diff1_masks\", len(changed_masks))\n",
        "for k,v in sorted([(k,v) for k,v in changed_masks.items()], key=lambda x: x[1], reverse=True)[:2000]:\n",
        "  print(np.binary_repr(k[0], width=16), np.binary_repr(k[1], width=16), np.binary_repr(k[2], width=16), v, v/num_samples_total)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xRJuBSuUJ3d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_changed_masks = sorted([(k,v) for k,v in changed_masks.items()], key=lambda x: x[1], reverse=True)[:1028]\n",
        "len(selected_changed_masks), sum(v for k,v in selected_changed_masks), sum(v for k,v in selected_changed_masks) / num_samples_total"
      ],
      "metadata": {
        "id": "myZiQOiXRTCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_changed_masks = [((-1,-1,-1),144835626)] + selected_changed_masks"
      ],
      "metadata": {
        "id": "Grr5Ev8S_HM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "\n",
        "def get_changed_masks_4b(samples):\n",
        "  for i, s in enumerate(samples):\n",
        "    if i == 0:\n",
        "      continue\n",
        "    m = samples[i-1] ^ s\n",
        "    m0 = (m & 0xF000, s & m & 0xF000)\n",
        "    m1 = (m & 0x0F00, s & m & 0x0F00)\n",
        "    m2 = (m & 0x00F0, s & m & 0x00F0)\n",
        "    m3 = (m & 0x000F, s & m & 0x000F)\n",
        "    yield (m0, m1, m2, m3)\n",
        "\n",
        "def count_all_changed_masks_4b():\n",
        "    num_total = 0\n",
        "    count_m0 = defaultdict(int)\n",
        "    count_m1 = defaultdict(int)\n",
        "    count_m2 = defaultdict(int)\n",
        "    count_m3 = defaultdict(int)\n",
        "\n",
        "    for fname in [\"d40b3d0a-21fd-42a8-a0bd-a38a431e9401.wav\"]:#os.listdir(\"data\"):\n",
        "      sample_rate, samples = wavfile.read(os.path.join(\"data\", fname))\n",
        "      for m0,m1,m2,m3 in get_changed_masks_4b(samples):\n",
        "        num_total += 1\n",
        "        count_m0[m0] += 1\n",
        "        count_m1[m1] += 1\n",
        "        count_m2[m2] += 1\n",
        "        count_m3[m3] += 1\n",
        "\n",
        "    return num_total, (count_m0, count_m1, count_m2, count_m3)\n",
        "\n",
        "num_total, changed_masks_4b = count_all_changed_masks_4b()\n",
        "\n",
        "for i in range(4):\n",
        "  changed_masks = changed_masks_4b[i]\n",
        "  print(f\"num_diff1_4bmasks: {i}\", len(changed_masks))\n",
        "  for k,v in sorted([(k,v) for k,v in changed_masks.items()], key=lambda x: x[1], reverse=True):\n",
        "    print(np.binary_repr(k[0], width=16), np.binary_repr(k[1], width=16), v, v/num_total)"
      ],
      "metadata": {
        "id": "qcwXRSV0OnJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Huffman encoding and decoding. Requires Python >= 3.7.\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from heapq import heapify\n",
        "from heapq import heappush\n",
        "from heapq import heappop\n",
        "\n",
        "from itertools import chain\n",
        "from itertools import islice\n",
        "\n",
        "from typing import BinaryIO\n",
        "from typing import Dict\n",
        "from typing import Iterable\n",
        "from typing import Optional\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "LEFT_BIT = \"0\"\n",
        "RIGHT_BIT = \"1\"\n",
        "WORD_SIZE = 8  # Assumed to be a multiple of 8.\n",
        "READ_SIZE = WORD_SIZE // 8\n",
        "P_EOF = 1 << WORD_SIZE\n",
        "\n",
        "\n",
        "class Node:\n",
        "    \"\"\"Huffman tree node.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        weight: int,\n",
        "        symbol: Optional[int] = None,\n",
        "        left: Optional[Node] = None,\n",
        "        right: Optional[Node] = None,\n",
        "    ):\n",
        "        self.weight = weight\n",
        "        self.symbol = symbol\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "\n",
        "    def is_leaf(self) -> bool:\n",
        "        \"\"\"Return `True` if this node is a leaf node, or `False` otherwise.\"\"\"\n",
        "        return self.left is None and self.right is None\n",
        "\n",
        "    def __lt__(self, other: Node) -> bool:\n",
        "        return self.weight < other.weight\n",
        "\n",
        "\n",
        "def huffman_tree(weights: Dict[int, int]) -> Node:\n",
        "    \"\"\"Build a prefix tree from a map of symbol frequencies.\"\"\"\n",
        "    heap = [Node(v, k) for k, v in weights.items()]\n",
        "    heapify(heap)\n",
        "\n",
        "    # Pseudo end-of-file with a weight of 1.\n",
        "    heappush(heap, Node(1, P_EOF))\n",
        "\n",
        "    while len(heap) > 1:\n",
        "        left, right = heappop(heap), heappop(heap)\n",
        "        node = Node(weight=left.weight + right.weight, left=left, right=right)\n",
        "        heappush(heap, node)\n",
        "\n",
        "    return heappop(heap)\n",
        "\n",
        "\n",
        "def huffman_table(tree: Node) -> Dict[int, str]:\n",
        "    \"\"\"Build a table of prefix codes by visiting every leaf node in `tree`.\"\"\"\n",
        "    codes: Dict[int, str] = {}\n",
        "\n",
        "    def walk(node: Optional[Node], code: str = \"\"):\n",
        "        if node is None:\n",
        "            return\n",
        "\n",
        "        if node.is_leaf():\n",
        "            assert node.symbol\n",
        "            codes[node.symbol] = code\n",
        "            return\n",
        "\n",
        "        walk(node.left, code + LEFT_BIT)\n",
        "        walk(node.right, code + RIGHT_BIT)\n",
        "\n",
        "    walk(tree)\n",
        "    return codes\n",
        "\n",
        "def _decode(bits: Iterable[str], tree: Node) -> Iterable[int]:\n",
        "    node = tree\n",
        "\n",
        "    for bit in bits:\n",
        "        if bit == LEFT_BIT:\n",
        "            assert node.left\n",
        "            node = node.left\n",
        "        else:\n",
        "            assert node.right\n",
        "            node = node.right\n",
        "\n",
        "        if node.symbol == P_EOF:\n",
        "            break\n",
        "\n",
        "        if node.is_leaf():\n",
        "            assert node.symbol\n",
        "            yield node.symbol\n",
        "            node = tree  # Back to the top of the tree."
      ],
      "metadata": {
        "id": "atpIK-MF4P8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = huffman_tree(sample_unique_counts)\n",
        "table = huffman_table(tree)\n",
        "print(\"len huffman table\", len(table))\n",
        "print(f\"Symbol Code\\n------ ----\")\n",
        "for k, v in sorted(table.items(), key=lambda x: len(x[1])):\n",
        "    print(np.binary_repr(k, width=16), v)"
      ],
      "metadata": {
        "id": "6DLlFVeCXhCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq = [(2786, 34), (1504, 34), (1761, 30), (2529, 28), (2273, 27), (2978, 24), (2209, 24), (2401, 23), (2722, 22), (1248, 20), (2081, 19), (1889, 17), (2337, 17), (1184, 16), (2914, 16), (2465, 15), (736, 15), (1697, 14), (2593, 14), (3042, 14), (1953, 13), (2145, 12), (3106, 11), (1825, 11), (1440, 11), (2017, 10), (864, 10), (1056, 10), (1633, 10), (1569, 10), (1312, 10), (1376, 10), (3170, 9), (3234, 9), (992, 8), (2850, 7), (3490, 7), (928, 7), (2658, 7), (3298, 7), (800, 6), (3426, 6), (3554, 6), (1120, 5), (287, 4), (415, 4), (479, 4), (544, 3), (3362, 3), (3683, 3), (3618, 2), (672, 2), (351, 2), (-545, 2), (223, 2), (4003, 2), (3811, 2), (3747, 2), (608, 1), (4131, 1), (95, 0), (159, 0), (-160, 0), (3939, 0), (4259, 0), (4451, 0), (3875, 0)]"
      ],
      "metadata": {
        "id": "0iz-8NQw2bj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = huffman_tree({v[0]: v[1] for v in freq})\n",
        "table = huffman_table(tree)\n",
        "print(\"len huffman table\", len(table))\n",
        "print(f\"Symbol Code\\n------ ----\")\n",
        "for k, v in sorted(table.items(), key=lambda x: len(x[1])):\n",
        "    print(k, v)"
      ],
      "metadata": {
        "id": "stpnwKvI4U55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "def get_is_match_prev(samples, n_prev: int = 256):\n",
        "  for i, s in enumerate(samples):\n",
        "    if i < n_prev + 1:\n",
        "      yield 0\n",
        "      continue\n",
        "\n",
        "    is_match = False\n",
        "    for j in range(n_prev):\n",
        "      if s == samples[i - n_prev - j]:\n",
        "        is_match = True\n",
        "\n",
        "    yield 1 if is_match else 0\n",
        "\n",
        "def get_is_match_prev_one(fname: str, n_prev: int = 256):\n",
        "    sample_rate, samples = wavfile.read(os.path.join(\"data\", fname))\n",
        "    for (s, is_match) in zip(samples, get_is_match_prev(samples, n_prev)):\n",
        "      yield s, is_match\n",
        "\n",
        "def count_is_match_prev_one(s):\n",
        "  sum = 0\n",
        "  total = 0\n",
        "  for _, is_match in s:\n",
        "    total += 1\n",
        "    sum += is_match\n",
        "  return sum, total\n",
        "\n",
        "def lengths_is_match_prev_one(s):\n",
        "  lengths0 = []\n",
        "  lengths1 = []\n",
        "\n",
        "  prev = -1\n",
        "  count = 0\n",
        "  for _, is_match in s:\n",
        "    if prev == -1:\n",
        "      prev = is_match\n",
        "      count = 1\n",
        "      continue\n",
        "\n",
        "    if is_match == prev:\n",
        "      count += 1\n",
        "    else:\n",
        "      if is_match == 1:\n",
        "        lengths1.append(count)\n",
        "      else:\n",
        "        lengths0.append(count)\n",
        "      prev = is_match\n",
        "      count = 1\n",
        "\n",
        "  return np.array(lengths0), np.array(lengths1)\n",
        "\n",
        "n_prev = 128\n",
        "print(\"n_prev\", n_prev)\n",
        "df_is_match = list(get_is_match_prev_one(\"d40b3d0a-21fd-42a8-a0bd-a38a431e9401.wav\", n_prev=n_prev))\n",
        "\n",
        "num_matched, total = count_is_match_prev_one(df_is_match)\n",
        "print(\"num_matched\", num_matched, \"num_total\", num_total, \"ratio_matched\", num_matched / num_total)\n",
        "for i,(s,v) in enumerate(df_is_match[:1000]):\n",
        "  print(i, np.binary_repr(s, width=16), v)"
      ],
      "metadata": {
        "id": "GVP0FjFFt7JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import plotly.express as px\n",
        "\n",
        "for n_prev in [2 ** n for n in range(12)]:\n",
        "  df_is_match = list(get_is_match_prev_one(\"d40b3d0a-21fd-42a8-a0bd-a38a431e9401.wav\", n_prev=n_prev))\n",
        "  num_matched, total = count_is_match_prev_one(df_is_match)\n",
        "  compression_ratio = 16 / ((1 - num_matched / num_total) * (16 + 1) + num_matched / num_total * (math.log2(n_prev) + 1))\n",
        "\n",
        "  print(0)\n",
        "  fig = px.histogram(lengths_is_match_prev_one(df_is_match)[0])#, histnorm='probability density')\n",
        "  fig.show()\n",
        "\n",
        "  print(1)\n",
        "  fig = px.histogram(lengths_is_match_prev_one(df_is_match)[1])#, histnorm='probability density')\n",
        "  fig.show()\n",
        "\n",
        "  print(\"n_prev\", n_prev, \"num_matched\", num_matched, \"num_total\", num_total, \"ratio_matched\", num_matched / num_total, \"compression_ratio\", compression_ratio)"
      ],
      "metadata": {
        "id": "oHnwbZh5yB65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from scipy.io import wavfile\n",
        "\n",
        "fname = \"b4a354ca-8194-4459-b711-0fd099b117e8.wav\"\n",
        "sample_rate, samples = wavfile.read(os.path.join(\"data\", fname))\n",
        "\n",
        "num_uncompressed_bytes = len(samples) * 16\n",
        "print(\"num_uncompressed_bytes\", num_uncompressed_bytes)"
      ],
      "metadata": {
        "id": "9ELvfkNZz1mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class Encoder():\n",
        "  def __init__(self, cache_size: int = 128, max_dictionary_use_len: int = 1024, min_dictionary_use_len: int = 8, log_first_n_samples: int = 1000):\n",
        "    self.cache_size = cache_size\n",
        "    self.max_dictionary_use_len = max_dictionary_use_len\n",
        "    self.log_first_n_samples = log_first_n_samples\n",
        "    self.min_dictionary_use_len = min_dictionary_use_len\n",
        "    self.cache = dict()\n",
        "    self.buffer = []\n",
        "\n",
        "  def __flush(self):\n",
        "    if len(self.buffer) == 0:\n",
        "      return\n",
        "\n",
        "    if len(self.buffer) < self.min_dictionary_use_len:\n",
        "      for s in self.buffer:\n",
        "          q = np.binary_repr(s, width=16)\n",
        "          self.__log(f\"{q}: <- {np.binary_repr(s, width=16)}: {len(q)/16:.2f} raw sample (buffer_len({len(self.buffer)}))\")\n",
        "          yield q\n",
        "\n",
        "    marker = \"1\" + np.binary_repr(len(self.buffer), width=int(math.log2(self.max_dictionary_use_len)))\n",
        "    self.__log(f\"{marker}: flush buffer, next n({len(self.buffer)}) samples are encoded with dictionary\")\n",
        "    yield marker\n",
        "\n",
        "    for s in self.buffer:\n",
        "      q = np.binary_repr(self.__get_cache_idx(s), width=int(math.log2(self.cache_size)))\n",
        "      self.cache[s] += 1\n",
        "      self.__log(f\"{q}: <- {np.binary_repr(s, width=16)}: {len(q)/16:.2f}\")\n",
        "      yield q\n",
        "\n",
        "    self.buffer = []\n",
        "\n",
        "  def __evict_from_cache(self):\n",
        "    # least frequently used cache eviction policy\n",
        "    min_k, min_v = None, None\n",
        "    for k,v in self.cache.items():\n",
        "      if min_v is None or v < min_v:\n",
        "        min_k, min_v = k, v\n",
        "    del self.cache[min_k]\n",
        "\n",
        "  def __add_to_cache(self, v: np.int16):\n",
        "    if len(self.cache) > self.cache_size:\n",
        "      self.__evict_from_cache()\n",
        "    self.cache[v] = 0\n",
        "\n",
        "  def __get_cache_idx(self, s: np.int16) -> int:\n",
        "    for i,(k,v) in enumerate(sorted([(k,v) for k,v in self.cache.items()], key=lambda x: x[1], reverse=True)):\n",
        "      if s == k:\n",
        "        return i\n",
        "    return None\n",
        "\n",
        "  def __log(self, s):\n",
        "    if self.log_first_n_samples < 0:\n",
        "      return\n",
        "    self.log_first_n_samples -= 1\n",
        "    print(s)\n",
        "\n",
        "  def encode(self, samples: np.array):\n",
        "    # we do not transfer dictionary, decoder has same algorithm, decoder reconstructs dictionary on its own from same sequence\n",
        "    for i,s in enumerate(samples):\n",
        "        if s in self.cache:\n",
        "          if len(self.buffer) >= self.max_dictionary_use_len:\n",
        "            for q in self.__flush():\n",
        "              yield q\n",
        "\n",
        "          self.buffer.append(s)\n",
        "          continue\n",
        "\n",
        "        for q in self.__flush():\n",
        "          yield q\n",
        "\n",
        "        self.__add_to_cache(s)\n",
        "\n",
        "        raw_sample = np.binary_repr(s, width=16)\n",
        "        self.__log(f\"{raw_sample}: <- {np.binary_repr(s, width=16)}: {len(raw_sample)/16:.2f} raw sample\")\n",
        "        yield raw_sample\n",
        "\n",
        "encoded = list(Encoder(cache_size=2**6,max_dictionary_use_len=2**15, min_dictionary_use_len=4).encode(samples))\n",
        "compressed_bytes = sum(len(s) for s in encoded)\n",
        "print(\"compressed_bytes\", compressed_bytes, \"compression_ratio\", num_uncompressed_bytes / compressed_bytes, \"num_samples_encoded\", len(encoded), \"num_samples\", len(samples))"
      ],
      "metadata": {
        "id": "7okGdHkwTFfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class Encoder2():\n",
        "  def __init__(self, cache_size: int = 128, max_dictionary_use_len: int = 1024, min_dictionary_use_len: int = 8, log_first_n_samples: int = 1000):\n",
        "    self.cache_size = cache_size\n",
        "    self.max_dictionary_use_len = max_dictionary_use_len\n",
        "    self.log_first_n_samples = log_first_n_samples\n",
        "    self.min_dictionary_use_len = min_dictionary_use_len\n",
        "    self.cache = dict()\n",
        "    self.buffer = []\n",
        "\n",
        "  def __flush(self):\n",
        "    if len(self.buffer) == 0:\n",
        "      return\n",
        "\n",
        "    if len(self.buffer) < self.min_dictionary_use_len:\n",
        "      for s in self.buffer:\n",
        "          q = \"0\" + np.binary_repr(s, width=16)\n",
        "          self.__log(f\"{q}: <- {np.binary_repr(s, width=16)}: {len(q)/16:.2f} raw sample (buffer_len({len(self.buffer)}))\")\n",
        "          yield q\n",
        "\n",
        "    marker = \"1\" + np.binary_repr(len(self.buffer), width=int(math.log2(self.max_dictionary_use_len)))\n",
        "    self.__log(f\"{marker}: flush buffer, next n({len(self.buffer)}) samples are encoded with dictionary\")\n",
        "    yield marker\n",
        "\n",
        "    for s in self.buffer:\n",
        "      q = np.binary_repr(self.__get_cache_idx(s), width=int(math.log2(self.cache_size)))\n",
        "      self.cache[s] += 1\n",
        "      self.__log(f\"{q}: <- {np.binary_repr(s, width=16)}: {len(q)/16:.2f}\")\n",
        "      yield q\n",
        "\n",
        "    self.buffer = []\n",
        "\n",
        "  def __evict_from_cache(self):\n",
        "    # least frequently used cache eviction policy\n",
        "    min_k, min_v = None, None\n",
        "    for k,v in self.cache.items():\n",
        "      if min_v is None or v < min_v:\n",
        "        min_k, min_v = k, v\n",
        "    del self.cache[min_k]\n",
        "\n",
        "  def __add_to_cache(self, v: np.int16):\n",
        "    if len(self.cache) > self.cache_size:\n",
        "      self.__evict_from_cache()\n",
        "    self.cache[v] = 0\n",
        "\n",
        "  def __get_cache_idx(self, s: np.int16) -> int:\n",
        "    for i,(k,v) in enumerate(sorted([(k,v) for k,v in self.cache.items()], key=lambda x: x[1], reverse=True)):\n",
        "      if s == k:\n",
        "        return i\n",
        "    return None\n",
        "\n",
        "  def __log(self, s):\n",
        "    if self.log_first_n_samples < 0:\n",
        "      return\n",
        "    self.log_first_n_samples -= 1\n",
        "    print(s)\n",
        "\n",
        "  def encode(self, samples: np.array):\n",
        "    # we do not transfer dictionary, decoder has same algorithm, decoder reconstructs dictionary on its own from same sequence\n",
        "    for i,s in enumerate(samples):\n",
        "        if len(self.buffer) >= self.max_dictionary_use_len:\n",
        "          for q in self.__flush():\n",
        "            yield q\n",
        "\n",
        "        self.buffer.append(s)\n",
        "\n",
        "    for q in self.__flush():\n",
        "      yield q\n",
        "\n",
        "    for q in self.__flush():\n",
        "      yield q\n",
        "\n",
        "encoded = list(Encoder2(cache_size=2**7,max_dictionary_use_len=2**14, min_dictionary_use_len=8).encode(samples))\n",
        "compressed_bytes = sum(len(s) for s in encoded)\n",
        "print(\"compressed_bytes\", compressed_bytes, \"compression_ratio\", num_uncompressed_bytes / compressed_bytes)"
      ],
      "metadata": {
        "id": "AAzuaDqjWpby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations Log\n",
        "- all values do indeed fit into 10bits, there are only 1023 distinct values\n",
        "- 1024 electrodes (set/not-set electrode data per sample) is already compressed into some 10bit number, which of 1024 electrodes is firing, we do not know based on 10bit number\n",
        "- there are 1024 differnt values for samples\n",
        "- 10bit values are not lower-bits, for some reason higher bits are also set. this can mean there is either: A) reserved full space of 16bits or B) there is some structure to bits arrangement already\n",
        "- value of 0 is not used. looks like it is reserved\n",
        "- ~most frequent samples have contigious series of zeroes~\n",
        "- ~most frequent samples have many zeroes~\n",
        "- ~there are no unused bits among 16bit samples~\n",
        "- ~bits are set equally frequently, there is not \"dead\"-bits in samples~\n",
        "- to achieve 200x compression, reducing single sample 16bit to 10bit or lower would not do it. we need to compress cross-sample information\n",
        "- ~there is significant continuity in bits across samples in single sequence of samples. akin to columnar data types, column based compression may be useful. maybe even just transposing data and compressing that can be very significant reduction in size.~\n",
        "- ~max bits sequence is ~2K (both, zeroes and ones)~\n",
        "- ~by doing \"column based same value count encoding for sequences of 16+\" 30% of bits can be removed~\n",
        "- ~consecutive samples are not very different, many samples have only 4 bits difference. (note: does not make sense to repeat whole almost-previous sample all over again)~\n",
        "- in single file, out of 90K samples, therea are only 1.5K different transitions between samples. in whole dataset there is only 25K different masks. out of 630M samples, that is 0.00003968253968 of total samples. unlikely this is coincidence. this simple heuristic highlights the fact of possible transitions. there is strong fundamental causality between neuron spikes. certain neruons spike only before other neurons. this causality in simplest form is encoded in \"possible\" transitions.\n",
        "- there is 1K transitions that are used more than 1K times, which is 98% of samples\n",
        "- ~calculating differences from previous in words of 4 does not work at all for some of 16bits~\n",
        "- bits in number not meaningful, this is again due to copmression of 1028 into 10bit number already. what we can actually do is check if this number is the same or not the same compare to others. information is likely encoded in ordering of these numbers. likely they either: A) repeat patterns (oscilations?); B) transition in certian graph of possible transitions (oscilations?);. Some basic herusitic on most recently repeated values may work well.\n",
        "- keeping cache of last N bit words, and passing either new word or index in cache, is best at N=128 and gives 1.54 compression ratio"
      ],
      "metadata": {
        "id": "BixINEX8KFdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "* in WAV each sample is bit value\n",
        "* http://tiny.systems/software/soundProgrammer/WavFormatDocs.pdf\n",
        "* https://docs.python.org/3/library/wave.html\n",
        "* https://github.com/go-audio/wav\n",
        "* provided files wav encode in 16bit per sample (even though doc says 10bit resolution)\n",
        "* `scipy.io.wavfile.read` does not support 10bit resolution\n",
        "* https://en.wikipedia.org/wiki/Variable-length_code\n",
        "* https://en.wikipedia.org/wiki/Prefix_code\n",
        "* https://rosettacode.org/wiki/Huffman_coding#Python\n",
        "* https://golang.google.cn/src/compress/bzip2/huffman.go\n",
        "* https://iopscience.iop.org/article/10.1088/1741-2552/acf5a4"
      ],
      "metadata": {
        "id": "N-SDzVJuHFQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "crs = np.genfromtxt('compressed_ratios.csv', delimiter=',')\n",
        "crs.shape, crs[:10], crs[-10:]"
      ],
      "metadata": {
        "id": "3bbRSlMqdZ3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "df = pd.DataFrame({\"compresseion_ratio\": crs})\n",
        "fig = px.histogram(df, histnorm='probability density', x=\"compresseion_ratio\", nbins=250)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "-sYb43cc4LZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(-crs, 64)"
      ],
      "metadata": {
        "id": "n2HJmiOIf5fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[go.Histogram(x=-crs, cumulative_enabled=True, histnorm='probability', nbinsx=270)])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "rOBlbqkjeEky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5D-MmTE1e_iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "crs_6bit = np.genfromtxt('compressed_ratios_6bit.csv', delimiter=',')\n",
        "crs_6bit.shape, crs[:10], crs[-10:]"
      ],
      "metadata": {
        "id": "6nmcGeRoj0cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "df = pd.DataFrame({\"compresseion_ratio\": crs_6bit})\n",
        "fig = px.histogram(df, histnorm='probability density', x=\"compresseion_ratio\", nbins=250)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "2GmL5M84j0cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(-crs_6bit, 64)"
      ],
      "metadata": {
        "id": "oNFbzZNSj0cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[go.Histogram(x=-crs_6bit, cumulative_enabled=True, histnorm='probability', nbinsx=270)])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "MffNzV8Wj0cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "crs = np.genfromtxt('compressed_ratios.csv', delimiter=',')\n",
        "crs.shape, crs[:10], crs[-10:]"
      ],
      "metadata": {
        "id": "faUOt6Yba0ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "df = pd.DataFrame({\"compresseion_ratio\": crs})\n",
        "fig = px.histogram(df, histnorm='probability density', x=\"compresseion_ratio\", nbins=250)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "SbwHySxDa0ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(-crs, 65)"
      ],
      "metadata": {
        "id": "ivVvGixjbCn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[go.Histogram(x=-crs, cumulative_enabled=True, histnorm='probability', nbinsx=270)])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "PzBGqmTNbCn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4,6,8"
      ],
      "metadata": {
        "id": "pShymg3qbFI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "crs = np.genfromtxt('compressed_ratios.csv', delimiter=',')\n",
        "crs.shape, crs[:10], crs[-10:]"
      ],
      "metadata": {
        "id": "YQOxsefw9ASQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(-crs, 65)"
      ],
      "metadata": {
        "id": "9J7wn_5K9ASV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "df = pd.DataFrame({\"compression_ratio\": crs})\n",
        "fig = px.histogram(df, histnorm='probability density', x=\"compression_ratio\", nbins=250)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "8FncHsTA9ASU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "import plotly.express as px\n",
        "df = pd.DataFrame({\"compression_ratio\": crs})\n",
        "fig = px.ecdf(\n",
        "    df, x=\"compression_ratio\",\n",
        "    markers=True,\n",
        "    ecdfmode=\"complementary\",\n",
        "    title=\"Probability of Compression Ratio Higher-Than (ecdfmode='complementary')\",\n",
        "    marginal=\"histogram\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "utI-bsz39ASV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B7EYqexc9ELt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "crs = np.genfromtxt('compressed_ratios_zip.csv', delimiter=',')\n",
        "crs.shape, crs[:10], crs[-10:]"
      ],
      "metadata": {
        "id": "H3sFTpecIM64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(-crs, 65)"
      ],
      "metadata": {
        "id": "7FrMJhJMIM6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "df = pd.DataFrame({\"compression_ratio\": crs})\n",
        "fig = px.histogram(df, histnorm='probability density', x=\"compression_ratio\", nbins=250)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "zbicawP1IM6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "import plotly.express as px\n",
        "df = pd.DataFrame({\"compression_ratio\": crs})\n",
        "fig = px.ecdf(\n",
        "    df, x=\"compression_ratio\",\n",
        "    markers=True,\n",
        "    ecdfmode=\"complementary\",\n",
        "    title=\"Probability of Compression Ratio Higher-Than (ecdfmode='complementary')\",\n",
        "    marginal=\"histogram\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "NySKRXAkIM7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8pqq6zm8IQNk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}